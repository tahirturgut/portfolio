# -*- coding: utf-8 -*-
"""HW2-CS412-NN-tahirturgut

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ihuajWfV2JxPXaj8SRzjre6B8bMiEdj3

# CS412 - Machine Learning - 2021
## Homework 2
100 pts


## Goal

The goal of this homework is two-fold:

*   Gain experience with neural network approaches
*   Gain experience with the Keras library

## Dataset
You are going to use a house price dataset that we prepared for you, that contains four independent variables (predictors) and one target variable. The task is predicting the target variable (house price) from the predictors (house attributes).


Download the data from SuCourse. Reserve 10% of the training data for validation and use the rest for development (learning your models). The official test data we provide (1,200 samples) should only be used for testing at the end, and not model selection.

## Task 
Build a regressor with a neural network that has only one hidden layer, using the Keras library function calls to predict house prices in the provided dataset.

Your code should follow the given skeleton and try the indicated parameters.

## Preprocessing and Meta-parameters
You should try 10,50 and 100 as hidden node count. 

You should  decide on the learning rate (step size), you can try values such as 0.001, 0.01, 0.1, but you may need to increase if learning is very slow or decrease if you see the loss increase!

You can use either sigmoid or Relu activations for the hidden nodes (indicate with your results) and you should know what to use for the activation for the output layer, input, output layer sizes, and the suitable loss function. 

## Software: 

Keras is a library that we will use especially for deep learning, but also with basic neural network functionality of course.

You may find the necessary function references here: 

http://scikit-learn.org/stable/supervised_learning.html
https://keras.io/api/

When you search for Dense for instance, you should find the relevant function and explained parameters, easily.

## Submission: 

Fill this notebook. Write the report section at the end.

You should prepare a separate pdf document as your homework (name hw1-CS412-yourname.pdf) which consists of the report (Part 8) of the notebook for easy viewing -and- include a link to your notebook from within the pdf report (make sure to include the link obtained from the #share link on top right, **be sure to share with Sabancı University first** as otherwise there will be access problems.).

##1) Initialize

*   First make a copy of the notebook given to you as a starter.

*   Make sure you choose Connect form upper right.

## 2) Load training dataset

* Load the datasets (train.csv, test.csv) provided on SuCourse on your Google drive and read the datasets using Google Drive's mount functions. 
You may find the necessary functions here: 
https://colab.research.google.com/notebooks/io.ipynb
"""

from google.colab import drive
drive.mount('/content/drive/') 
# click on the url that pops up and give the necessary authorizations

"""

*   Set your notebooks working directory to the path where the datasets are uploaded (cd is the linux command for change directory) 
*   You may need to use cd drive/MyDrive depending on your path to the datasets on Google Drive. (don't comment the code in the cells when using linux commands)




"""

cd drive/MyDrive

"""* List the files in the current directory."""

ls

"""##3) Understanding the dataset

There are alot of functions that can be used to know more about this dataset

- What is the shape of the training set (num of samples X number of attributes) ***[shape function can be used]***

- Display attribute names ***[columns function can be used]***

- Display the first 5 rows from training dataset ***[head or sample functions can be used]***

..
"""

# import the necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

train_df = pd.read_csv("train.csv")
# show first 10 elements of the training data
train_df.head(10)

#attribute names
print(train_df.columns)

# print the shape of data

print("Data dimensionality is:", train_df.shape)
print("------------------------")

# also give some statistics about the data like mean, standard deviation etc.

print("Data mean for each column:")
print(train_df.mean())
print("------------------------")
print("Data standart deviation for each column:")
print(train_df.std())

"""##4) Preprocessing Steps

As some of the features (predictive variables) on this dataset are categorical (non-numeric) you need to do some preprocessing for those features.

Please read 7-features.pdf under SuCourse for converting categorical variables to dummy variables. You can use as many **dummy or indicator variables** as there are categories within one feature. You can also look at pandas' get_dummies or keras.utils.to_categorical functions.

In neural networks, scaling of the features are important, because they affect the net input of a neuron as a whole. You should use **MinMax scaler** on sklearn for this task, which scales the variables between 0 and 1 on by default. (Remember that mean-squared error loss function tends to be extremely large with unscaled features.)

"""

# encode the categorical variables
train_df = pd.get_dummies(train_df, prefix=['view', 'crime_rate'], columns=['view', 'crime_rate'])
print(train_df.columns)

# scale the features between 0-1
from sklearn.preprocessing import MinMaxScaler
msc = MinMaxScaler(feature_range=(0, 1))

scaled_train = msc.fit_transform(train_df)
scaled_train_df = pd.DataFrame(scaled_train, columns=train_df.columns.values)

"""Don't forget the split the training data to obtain a validation set. **Use random_state=42**"""

from sklearn.utils import shuffle

# Shuffle the training data
scaled_train_df = shuffle(scaled_train_df, random_state = 20)

# features to be used to predict and feature to be predicted
scaled_train_df_X = scaled_train_df.drop(["price"], axis=1)
scaled_train_df_Y = scaled_train_df['price']

# split 90-10
from sklearn.model_selection import train_test_split

X_train, X_validation, Y_train, Y_validation = train_test_split(scaled_train_df_X, scaled_train_df_Y, test_size=0.1, random_state = 42)

"""##5) Train neural networks on development data and do model selection using the validation data


* Train a neural network with **one hidden layer** (try 3 different values for the number of neurons in that hidden layer, as 25, 50, 100), you will need to correctly choose the optimizer and the loss function that this model will train with. Use batch_size as 64 and train each model for 30 epochs. 

* Train another neural network with two hidden layers with meta-parameters of your choice. Again, use batch_size as 64 and train the model for 30 epochs. 

* **Bonus (5 pts)** Train a KNN or a Decision Tree model with your own choice of meta parameters to predict the house prices.

"""

from sklearn.tree import DecisionTreeRegressor

# develop the model with specified parameters
decision_tree = DecisionTreeRegressor(max_depth=10, min_samples_split=200)
decision_tree.fit(X_train, Y_train)

# make the predictions on the validation set
dt_predictions = decision_tree.predict(X_validation)

# calculate the mean squared error of predictions
from sklearn.metrics import mean_squared_error
score = mean_squared_error(Y_validation, dt_predictions)
print(score)

import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
# train one-hidden layered neural networks
# define your model architecture
model1 = Sequential()
model1.add(Dense(100, activation='sigmoid', input_shape=(8,)))
model1.add(Dense(1, name='output'))

model2 = Sequential()
model2.add(Dense(50, activation='sigmoid', input_shape=(8,)))
model2.add(Dense(1, name='output'))

model3 = Sequential()
model3.add(Dense(25, activation='sigmoid', input_shape=(8,)))
model3.add(Dense(1, name='output'))

# compile your model with an optimizer
model1.compile(loss='mean_squared_error', optimizer=Adam())
model2.compile(loss='mean_squared_error', optimizer=Adam())
model3.compile(loss='mean_squared_error', optimizer=Adam())

# fit the model on training data
model1.fit(X_train, Y_train, batch_size=64, epochs=30, shuffle=True, verbose=1)
model2.fit(X_train, Y_train, batch_size=64, epochs=30, shuffle=True, verbose=1)
model3.fit(X_train, Y_train, batch_size=64, epochs=30, shuffle=True, verbose=1)

# train a two-hidden layered neural network
model_2layered = Sequential()
model_2layered.add(Dense(64, activation='sigmoid', input_shape=(8,)))
model_2layered.add(Dense(32, activation='sigmoid'))
model_2layered.add(Dense(1, name='output'))

model_2layered.compile(loss='mean_squared_error', optimizer=Adam())

model_2layered.fit(X_train, Y_train, batch_size=64, epochs=30, shuffle=True, verbose=1)

"""## 6) Test your trained classifiers on the Validation set
Test your trained classifiers on the validation set and print the mean squared errors.

"""

# tests on validation
score1 = model1.evaluate(X_validation, Y_validation, verbose=0)
print('Validation loss of model1:', score1)

score2 = model2.evaluate(X_validation, Y_validation, verbose=0)
print('Validation loss of model2:', score2)

score3 = model3.evaluate(X_validation, Y_validation, verbose=0)
print('Validation loss of model3:', score3)

score_2layer = model_2layered.evaluate(X_validation, Y_validation, verbose=0)
print('Validation loss of model with 2 layers:', score_2layer)

"""## 7) Test your classifier on Test set

- Load test data
- Apply same pre-processing as training data (encoding categorical variables, scaling)
- Predict the labels of testing data **using the best model that you have selected according to your validation results** and report the mean squared error. 
"""

test_df = pd.read_csv("test.csv")

# encode the categorical variables
test_df = pd.get_dummies(test_df, prefix=['view', 'crime_rate'], columns=['view', 'crime_rate'])

# scale the features between 0-1
msc = MinMaxScaler(feature_range=(0, 1))

scaled_test = msc.fit_transform(test_df)
scaled_test_df = pd.DataFrame(scaled_test, columns=test_df.columns.values)

# features to be used to predict and feature to be predicted
scaled_test_df_X = scaled_test_df.drop(["price"], axis=1)
scaled_test_df_Y = scaled_test_df['price']

# test results
score_2layer = model_2layered.evaluate(scaled_test_df_X, scaled_test_df_Y, verbose=0)
print('Test loss of model with 2 layers:', score_2layer)

"""##8) Report Your Results

**Notebook should be RUN:** As training and testing may take a long time, we may just look at your notebook results without running the code again; so make sure **each cell is run**, so outputs are there.

**Report:** Write an **1-2 page summary** of your approach to this problem **below**. 

**Must include statements such as those below:**
**(Remove the text in parentheses, below, and include your own report)**

( Include the problem definition: 1-2 lines )

 (Talk about train/val/test sets, size and how split. )

 (Talk about feature extraction or preprocessing.)

**Add your observations as follows** (keep the questions for easy grading/context) in the report part of your notebook.

**Observations**

- Try a few learning rates for N=25 hidden neurons,  train for the indicated amount of epochs. Comment on what happens when learning rate is large or small? What is a good number/range for the learning rate?
Your answer here….

- Use that learning rate and vary the number of hidden neurons for the given values and try the indicated number of epochs. Give the validation mean squared errors for different approach and meta-parameters tried **in a table** and state which one you selected as your model. How many hidden neurons give the best model? 
Your answer here….

- State  what your test results are with the chosen approach and meta-parameters: e.g. "We have obtained the best results on the validation set with the ..........approach using a value of ...... for .... parameter. The result of this model on the test data is ..... % accuracy."" 

- How slow is learning? Any other problems?
Your answer here….

- Any other observations (not needed)

 You can add additional visualization as separate pages if you want, think of them as appendix, keeping the summary to 1-2-pages.


"""

