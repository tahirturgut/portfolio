# -*- coding: utf-8 -*-
"""CS412 PROJECT adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12mAb_oFkeWFOrD7WdWeYdRe4FmOKtrCA
"""

from google.colab import drive
from os.path import join
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import keras
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from keras import Sequential 
from keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD
import math as mth
from matplotlib import pyplot as plt

drive.mount('./drive')
path_prefix = "./drive/MyDrive"

def get_null_row_count(train):
  is_NaN = train.isnull()
  row_has_NaN = is_NaN.any(axis=1)
  rows_with_NaN = train[row_has_NaN]
  print(len(rows_with_NaN))

test = pd.read_csv(join(path_prefix, "test.csv"))
train = pd.read_csv(join(path_prefix, "train.csv"))


test.set_index("ID", inplace=True)
train.set_index("ID", inplace=True)
print(train.shape)
print(test.shape)

#drop the nan values, since they are neglectable
train = train.dropna(how="any", subset = ['brand', 'model', 'year', 'transmission', 'mileage', 'fuelType', 'mpg', 'engineSize'])
print(train.shape)

y_train = train['price']
train.drop(labels="price", axis=1, inplace=True)

#combine train and test
train_test = train.append(test)

#since tax column is the bottleneck for dropna function, we filled na's with tax(£) column
train_test['tax'].fillna((train_test['tax(£)']*0.74), inplace=True)

#drop the column with no data
train_test = train_test.drop(['tax(£)'], axis = 1)

get_null_row_count(train_test)

#create dic for matching brands and models
models={}
brands={}
for row in train_test.iterrows():
  brand=row[1][0]
  model=row[1][1]
  if model not in models.keys():
    models[model]=brand

  if brand in brands.keys():
    if model in brands[brand].keys():
      brands[brand][model]+=1
    else:
      brands[brand][model]=1
  else:
    brands[brand]={}

brands_max={}
for element in brands.keys():
 brands_max[element] = max(brands[element], key=brands[element].get)

#fill nan brands with correct brand
train_test['brand'].fillna(train_test.model.map(models), inplace=True)

#fill nan models with most likely model name
train_test['model'].fillna(train_test.brand.map(brands_max), inplace=True)

#fill numeric nan values with mean
train_test = train_test.fillna(train_test.mean())

#fill categoric nan values with mode
train_test = train_test.fillna(train_test.mode().iloc[0])

get_null_row_count(train_test)

#change year to age
train_test['year'] = 2021-train_test['year'] 
train_test.rename({'year': 'age'},axis=1,inplace=True)

print(train_test.head(10))

get_null_row_count(train_test)

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
 
# Encode labels in column 'species'.
train_test['brand']= label_encoder.fit_transform(train_test['brand'])
train_test['model']= label_encoder.fit_transform(train_test['model'])
train_test['transmission']= label_encoder.fit_transform(train_test['transmission'])
train_test['fuelType']= label_encoder.fit_transform(train_test['fuelType'])

#train_test = pd.get_dummies(train_test, columns=['brand','model','transmission','fuelType'])

#seperate train and test
X_train = train_test[0:59514]
X_test = train_test[59514:]

print(train_test.head(300))

from sklearn.model_selection import train_test_split
X_train_sub, X_validation_sub, y_train_sub, y_validation_sub = train_test_split(X_train, y_train, random_state=0)

print("X_train_sub: ",X_train_sub.shape)
print("y_train_sub: :",y_train_sub.shape)
print()
print("X_validation_sub: ",X_validation_sub.shape)
print("y_validation_sub: ",y_validation_sub.shape)

#random forest son hali
from sklearn.ensemble import RandomForestRegressor

#rf = RandomForestRegressor(n_estimators=300, max_depth=15)
#rf = RandomForestRegressor(n_estimators=80)
rf = RandomForestRegressor(n_estimators=170, max_depth=14)

#n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, 
#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, 
#min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, 
#verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None

rf.fit(X_train_sub, y_train_sub)
r2_train = rf.score(X_train_sub, y_train_sub)
print("r2 score of train", r2_train)
r2_valid = rf.score(X_validation_sub, y_validation_sub)
print("r2 score of validation", r2_valid)

predictions = rf.predict(X_test)
output_array = []
for i in range(len(predictions)):
  output_array.append(int(predictions[i]))
  print(i,"prediction:", int(predictions[i]))

print(max(output_array))
print(min(output_array))
print(sum(output_array)/len(output_array))

myDict = {'price': output_array} 
output = pd.DataFrame(myDict)
output.to_csv("son6.csv")

fig, axs = plt.subplots(1, 5)
fig.set_size_inches(22.5, 5.5, forward=True)
axs[0].scatter(X_test_scale['mileage'], predict_test1, c='crimson')
axs[1].scatter(X_test_scale['mpg'], predict_test1, c='crimson')
axs[2].scatter(X_test_scale['engineSize'], predict_test1, c='crimson')
axs[3].scatter(X_test_scale['tax'], predict_test1, c='crimson')
axs[4].scatter(X_test_scale['year'], predict_test1, c='crimson')
plt.show()